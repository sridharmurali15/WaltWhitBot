# -*- coding: utf-8 -*-
"""poem_ai

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q8sJMyrgHsAdbYWP6r4AgCN4LAipVtNV
"""

import keras
import numpy as np
import tensorflow as tf
import nltk
from keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.utils import to_categorical
import pickle
import pandas as pd

# read file with walt_whitman's poems
# reading just first 100,000 characters for computing purposes
file = open('walt_whitman.txt',r)
data = file.readlines().lower()[:100000]

# identifying all the unique characters in the dataset
# punctuation marks are also used as it can provide a good level of information to the poem
chars = sorted(list(set(data)))

# creating dictionaries which map each character to a id and vice-versa
char_to_id = dict((c,i) for i,c in enumerate(chars))
id_to_char = dict((i,c) for i,c in enumerate(chars))
n_chars = len(chars)
n_data = len(data)

# saving the dictionaries as it will be used as a map when resuing th emodel
with open('char_to_id.pkl', 'wb') as f:
    pickle.dump(char_to_id, f)

with open('id_to_char.pkl', 'wb') as f:
    pickle.dump(id_to_char, f)
    
# building the dataset and and predicted variables
train_length = 100
X = []
Y = []
for i in range(0,n_data - train_length):
    X.append([char_to_id[char] for char in data[i:i+train_length]])
    Y.append(char_to_id[data[i+train_length]])
n_trainx = len(X)

# one hot encoding the predicted variable
y = to_categorical(Y)

# resizing the input variable
x = np.array(X).reshape(n_trainx,train_length,1)
x = x/n_chars

# building a deep neural network with 1 LSTM layer and a fully connceted layer
# a bidirectionaly LSTM layer was also attempted but could training time and computing power limited me from utilizing it
model = Sequential()
model.add(LSTM(700))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')

# model fit
model.fit(x,y,epochs=100,batch_size=128)

# test result with a random few words from the dataset
seed = np.random.randint(0,n_trainx-1)
bot_poem_vector = X[seed]
bot_poem = [id_to_char[i] for i in bot_poem_vector]
# a maximum 150 characters are preducted
char_limit = 150
for i in range(char_limit):
    botx = np.array(bot_poem_vector).reshape(1,train_length,1)
    botx = botx/n_chars
    pred_ind = np.argmax(model.predict(botx, verbose=0))
    pred_char = id_to_char[pred_ind]
    bot_poem_vector.append(pred_ind)
    bot_poem.append(pred_char)
    bot_poem_vector = bot_poem_vector[1:]
bot_poem_finish = "".join(bot_poem)
print(bot_poem_finish)
 
# saving model
with open('model', 'wb') as files:
    pickle.dump(model, files)

#from google.colab import files
#files.download('all_love_100k')

# testing result with a random input sequence
bot_poem = list('looking at the moon ')
bot_poem_vector = [char_to_id[char] for char in bot_poem]
bot_poem_vector =  [1]*(100-len(bot_poem_vector)) + bot_poem_vector
char_limit = 150
for i in range(char_limit):
    botx = np.array(bot_poem_vector).reshape(1,train_length,1)
    botx = botx/n_chars
    pred_ind = np.argmax(model.predict(botx, verbose=0))
    pred_char = id_to_char[pred_ind]
    bot_poem_vector.append(pred_ind)
    bot_poem.append(pred_char)
    bot_poem_vector = bot_poem_vector[1:]
bot_poem_finish = "".join(bot_poem)
print(bot_poem_finish)
